{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95870ee0",
      "metadata": {},
      "source": [
        "# DeepSeek-R1-Distill-Qwen-14B Execution Prediction Notebook\n",
        "\n",
        "This notebook evaluates **deepseek-ai/DeepSeek-R1-Distill-Qwen-14B**, a reasoning model. Following the paper, we provide the **zero-shot execution prediction and choice prompts**, and collect five generations per problem to compute OC/OR/MC/MR and execution-choice metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f710dc",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51bb42b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate datasets tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45758108",
      "metadata": {},
      "source": [
        "## Step 2: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1666fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "MODEL_ID = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B'\n",
        "print('Loading model:', MODEL_ID)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto',\n",
        ")\n",
        "device = next(model.parameters()).device\n",
        "print('\u2713 Model ready on', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa68a2e8",
      "metadata": {},
      "source": [
        "## Step 3: Load Benchmark Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a7f515",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_REPO_ID = \"asgaur/leetcode-contests-431-467-mutations-strat110\"\n",
        "SPLIT = \"train\"\n",
        "\n",
        "print(f\"Loading dataset from {DATASET_REPO_ID}...\")\n",
        "dataset = load_dataset(DATASET_REPO_ID)\n",
        "print(f\"Total samples: {len(dataset[SPLIT])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fc56ba6",
      "metadata": {},
      "source": [
        "## Step 4: Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51856d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import ast\n",
        "from typing import Dict\n",
        "\n",
        "ZERO_SHOT_EXECUTION_PROMPT = \"\"\"You are given a Python program and an assertion containing an input to a function. Replace the ?? in the assertion with a literal (no unsimplified expressions, no function calls) representing the function's return value for the given input. Execute the program exactly as written, even if it is incorrect or incomplete. Provide the full assertion in [ANSWER] and [/ANSWER] tags.\"\"\"\n",
        "\n",
        "ZERO_SHOT_CHOICE_PROMPT = \"\"\"Execution Choice Prompt (Zero-Shot)\n",
        "You are given two Python programs below and an assertion containing an input to a function. First, choose either program, whichever one you are more confident in reasoning about. Then, replace the ?? in the assertion with a literal (no unsimplified expressions, no function calls) representing the function's return value for the given input on your chosen program. Execute the program exactly as written, even if it is incorrect or incomplete. For your final answer, output the letter of your chosen program (A or B) and the full assertion in the following json format:\n",
        "{\n",
        "\"chosen_program\": chosen_program_letter,\n",
        "\"assertion\": full_assertion\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def build_execution_prediction_prompt(sample: Dict, use_mutated: bool = False) -> str:\n",
        "    function_name = sample['function_name']\n",
        "    code_key = 'mutated_code' if use_mutated else 'code'\n",
        "    program = sample.get(code_key)\n",
        "    test_input = sample['input']\n",
        "    if test_input and test_input.startswith(f\"{function_name}(\") and test_input.endswith(\")\"):\n",
        "        input_args = test_input[len(function_name) + 1:-1]\n",
        "    else:\n",
        "        input_args = test_input\n",
        "    return f\"{ZERO_SHOT_EXECUTION_PROMPT}\n",
        "\n",
        "[PYTHON]\n",
        "{program}\n",
        "assert {function_name}({input_args}) == ??\n",
        "[/PYTHON]\"\n",
        "\n",
        "\n",
        "def build_execution_choice_prompt(sample: Dict, original_first: bool = True):\n",
        "    function_name = sample['function_name']\n",
        "    original_code = sample['code']\n",
        "    mutated_code = sample.get('mutated_code') or original_code\n",
        "    test_input = sample['input']\n",
        "    if test_input and test_input.startswith(f\"{function_name}(\") and test_input.endswith(\")\"):\n",
        "        input_args = test_input[len(function_name) + 1:-1]\n",
        "    else:\n",
        "        input_args = test_input\n",
        "    if original_first:\n",
        "        program_a, program_b = original_code, mutated_code\n",
        "        mapping = {'A': 'original', 'B': 'mutated'}\n",
        "    else:\n",
        "        program_a, program_b = mutated_code, original_code\n",
        "        mapping = {'A': 'mutated', 'B': 'original'}\n",
        "    question = f\"[PROGRAM_A]\n",
        "{program_a}\n",
        "[/PROGRAM_A]\n",
        "[PROGRAM_B]\n",
        "{program_b}\n",
        "[/PROGRAM_B]\n",
        "[ASSERTION]\n",
        "assert {function_name}({input_args}) == ??\n",
        "[/ASSERTION]\"\n",
        "    prompt = f\"{ZERO_SHOT_CHOICE_PROMPT}\n",
        "{question}\"\n",
        "    return prompt, mapping\n",
        "\n",
        "\n",
        "def parse_execution_choice_response(response: str) -> Dict[str, str]:\n",
        "    json_match = re.search(r'\\{\\s*\"chosen_program\".*?\\}', response, re.DOTALL)\n",
        "    if not json_match:\n",
        "        raise ValueError('Could not find JSON payload in response.')\n",
        "    json_text = json_match.group(0)\n",
        "    try:\n",
        "        return json.loads(json_text)\n",
        "    except json.JSONDecodeError:\n",
        "        chosen_match = re.search(r'\"chosen_program\"\\s*:\\s*\"?([A-Za-z])\"?', json_text)\n",
        "        assertion_match = re.search(r'\"assertion\"\\s*:\\s*(\"(?:[^\"\\]|\\.)*\")', json_text)\n",
        "        if not chosen_match or not assertion_match:\n",
        "            raise ValueError('Failed to parse execution choice JSON response.')\n",
        "        chosen_program = chosen_match.group(1)\n",
        "        assertion_literal = assertion_match.group(1)\n",
        "        try:\n",
        "            assertion = ast.literal_eval(assertion_literal)\n",
        "        except Exception:\n",
        "            assertion = assertion_literal.strip('\"')\n",
        "        return {'chosen_program': chosen_program, 'assertion': assertion}\n",
        "\n",
        "\n",
        "def extract_output_from_assertion(assertion: str) -> str:\n",
        "    if not assertion:\n",
        "        return ''\n",
        "    text = assertion.strip()\n",
        "    text = re.sub(r'^\\[ASSERTION\\]\\s*', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\s*\\[/ASSERTION\\]$', '', text, flags=re.IGNORECASE)\n",
        "    match = re.search(r'assert\\s+[\\w\\.]+\\([^)]*\\)\\s*==\\s*(.+)', text)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_answer_from_response(response: str) -> str:\n",
        "    pattern = r'\\[ANSWER\\](.*?)\\[/ANSWER\\]'\n",
        "    matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)\n",
        "    if matches:\n",
        "        assertion = matches[0].strip()\n",
        "        extracted = extract_output_from_assertion(assertion)\n",
        "        return extracted\n",
        "    pattern = r\"assert\\s+\\w+\\([^)]*\\)\\s*==\\s*(.+?)(?:\\n|$)\"\n",
        "    matches = re.findall(pattern, response, re.MULTILINE)\n",
        "    if matches:\n",
        "        return matches[0].strip()\n",
        "    return response.strip()\n",
        "\n",
        "def check_predicted_output(predicted_output: str, expected_output: str):\n",
        "    predicted = (predicted_output or '').strip()\n",
        "    expected = (expected_output or '').strip()\n",
        "    if predicted == expected:\n",
        "        return True, None\n",
        "    try:\n",
        "        predicted_val = ast.literal_eval(predicted)\n",
        "        expected_val = ast.literal_eval(expected)\n",
        "        if predicted_val == expected_val:\n",
        "            return True, None\n",
        "    except (ValueError, SyntaxError):\n",
        "        pass\n",
        "    return False, f\"Predicted: {predicted}, Expected: {expected}\"\n",
        "\n",
        "\n",
        "def is_boolean_output(value: str) -> bool:\n",
        "    if value is None:\n",
        "        return False\n",
        "    try:\n",
        "        parsed = ast.literal_eval(value.strip())\n",
        "        return isinstance(parsed, bool)\n",
        "    except (ValueError, SyntaxError, AttributeError):\n",
        "        lowered = value.strip().lower()\n",
        "        return lowered in {'true', 'false'}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c5e000",
      "metadata": {},
      "source": [
        "## Step 5: Test Execution Prediction on One Sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9121407a",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sample = dataset[SPLIT][0]\n",
        "prompt = build_execution_prediction_prompt(test_sample, use_mutated=False)\n",
        "print('Prompt:\n",
        "', prompt)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "encoded = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_dict=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "outputs = model.generate(\n",
        "    **encoded,\n",
        "    do_sample=True,\n",
        "    temperature=TEMPERATURE,\n",
        "    top_p=TOP_P,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        ")\n",
        "generated = outputs[0, encoded['input_ids'].shape[-1]:]\n",
        "response = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "print('\n",
        "Model response:\n",
        "', response)\n",
        "prediction = extract_answer_from_response(response)\n",
        "print('\n",
        "Parsed prediction:', prediction)\n",
        "print('Expected output :', test_sample['output'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71155889",
      "metadata": {},
      "source": [
        "## Step 6: Execution Prediction Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090d52fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "from typing import Dict, List\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "NUM_PROBLEMS = None\n",
        "START_INDEX = 0\n",
        "NUM_GENERATIONS = 5\n",
        "MAX_NEW_TOKENS = 800\n",
        "TEMPERATURE = 0.2\n",
        "TOP_P = 0.95\n",
        "SEED = 42\n",
        "SKIP_BOOLEAN_FOR_REVERSION = True\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def _compute_pass(counts: Dict[str, int]):\n",
        "    return counts['success'] / counts['total'] if counts['total'] else None\n",
        "\n",
        "def _decode_predictions(prompt: str, seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    encoded = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_dict=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "    start_time = time.time()\n",
        "    outputs = model.generate(\n",
        "        **encoded,\n",
        "        do_sample=True,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "    )\n",
        "    latency = time.time() - start_time\n",
        "    generated = outputs[0, encoded['input_ids'].shape[-1]:]\n",
        "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "    return response, latency\n",
        "\n",
        "\n",
        "split = dataset[SPLIT]\n",
        "stop = len(split) if NUM_PROBLEMS is None else min(len(split), START_INDEX + NUM_PROBLEMS)\n",
        "problem_indices = list(range(START_INDEX, stop))\n",
        "if not problem_indices:\n",
        "    raise ValueError('No problems selected. Adjust START_INDEX/NUM_PROBLEMS.')\n",
        "\n",
        "metrics_counts = {\n",
        "    'OC': {'success': 0, 'total': 0},\n",
        "    'OR': {'success': 0, 'total': 0},\n",
        "    'MC': {'success': 0, 'total': 0},\n",
        "    'MR': {'success': 0, 'total': 0},\n",
        "}\n",
        "reversion_skip_count = 0\n",
        "prediction_records: List[Dict] = []\n",
        "all_latencies: List[float] = []\n",
        "\n",
        "for idx in tqdm(problem_indices, desc='Execution prediction'):\n",
        "    sample = split[idx]\n",
        "    mutated_code = sample.get('mutated_code')\n",
        "    if not mutated_code:\n",
        "        prediction_records.append({'problem_index': int(idx), 'problem_id': sample.get('id'), 'skipped': True, 'skip_reason': 'missing mutated_code'})\n",
        "        continue\n",
        "\n",
        "    original_output = sample['output']\n",
        "    mutated_output = sample.get('mutated_output') or original_output\n",
        "    include_reversion = True\n",
        "    if SKIP_BOOLEAN_FOR_REVERSION and (is_boolean_output(original_output) or is_boolean_output(mutated_output)):\n",
        "        include_reversion = False\n",
        "        reversion_skip_count += 1\n",
        "\n",
        "    original_prompt = build_execution_prediction_prompt(sample, use_mutated=False)\n",
        "    mutated_prompt = build_execution_prediction_prompt(sample, use_mutated=True)\n",
        "\n",
        "    oc_successes = or_successes = mc_successes = mr_successes = 0\n",
        "    original_predictions = []\n",
        "    mutated_predictions = []\n",
        "\n",
        "    seed_base = SEED + idx * 1000\n",
        "    for gen_idx in range(NUM_GENERATIONS):\n",
        "        response_text, latency = _decode_predictions(original_prompt, seed_base + gen_idx)\n",
        "        all_latencies.append(latency)\n",
        "        original_prediction = extract_answer_from_response(response_text)\n",
        "        oc_correct, _ = check_predicted_output(original_prediction, original_output)\n",
        "        or_correct = None\n",
        "        if include_reversion:\n",
        "            or_correct, _ = check_predicted_output(original_prediction, mutated_output)\n",
        "        original_predictions.append({'generation': gen_idx, 'prediction': original_prediction, 'response': response_text, 'latency_s': latency, 'oc_correct': bool(oc_correct), 'or_correct': bool(or_correct) if isinstance(or_correct, bool) else None})\n",
        "        if oc_correct:\n",
        "            oc_successes += 1\n",
        "        if include_reversion and or_correct:\n",
        "            or_successes += 1\n",
        "\n",
        "        response_text_mut, latency_mut = _decode_predictions(mutated_prompt, seed_base + 500 + gen_idx)\n",
        "        all_latencies.append(latency_mut)\n",
        "        mutated_prediction = extract_answer_from_response(response_text_mut)\n",
        "        mc_correct, _ = check_predicted_output(mutated_prediction, mutated_output)\n",
        "        mr_correct = None\n",
        "        if include_reversion:\n",
        "            mr_correct, _ = check_predicted_output(mutated_prediction, original_output)\n",
        "        mutated_predictions.append({'generation': gen_idx, 'prediction': mutated_prediction, 'response': response_text_mut, 'latency_s': latency_mut, 'mc_correct': bool(mc_correct), 'mr_correct': bool(mr_correct) if isinstance(mr_correct, bool) else None})\n",
        "        if mc_correct:\n",
        "            mc_successes += 1\n",
        "        if include_reversion and mr_correct:\n",
        "            mr_successes += 1\n",
        "\n",
        "    metrics_counts['OC']['success'] += oc_successes\n",
        "    metrics_counts['OC']['total'] += NUM_GENERATIONS\n",
        "    metrics_counts['MC']['success'] += mc_successes\n",
        "    metrics_counts['MC']['total'] += NUM_GENERATIONS\n",
        "    if include_reversion:\n",
        "        metrics_counts['OR']['success'] += or_successes\n",
        "        metrics_counts['OR']['total'] += NUM_GENERATIONS\n",
        "        metrics_counts['MR']['success'] += mr_successes\n",
        "        metrics_counts['MR']['total'] += NUM_GENERATIONS\n",
        "\n",
        "    prediction_records.append({'problem_index': int(idx), 'problem_id': sample.get('id'), 'difficulty': sample.get('difficulty'), 'include_reversion': include_reversion, 'original_output': original_output, 'mutated_output': mutated_output, 'oc_successes': oc_successes, 'or_successes': or_successes if include_reversion else None, 'mc_successes': mc_successes, 'mr_successes': mr_successes if include_reversion else None, 'original_predictions': original_predictions, 'mutated_predictions': mutated_predictions})\n",
        "\n",
        "metrics_summary = {metric: _compute_pass(counts) for metric, counts in metrics_counts.items()}\n",
        "avg_latency = (sum(all_latencies) / len(all_latencies)) if all_latencies else None\n",
        "benchmark_summary = {\n",
        "    'dataset': DATASET_REPO_ID,\n",
        "    'problems_evaluated': len(problem_indices),\n",
        "    'generations_per_problem': NUM_GENERATIONS,\n",
        "    'oc_pass_at_1': metrics_summary['OC'],\n",
        "    'or_pass_at_1': metrics_summary['OR'],\n",
        "    'mc_pass_at_1': metrics_summary['MC'],\n",
        "    'mr_pass_at_1': metrics_summary['MR'],\n",
        "    'avg_latency_s': avg_latency,\n",
        "    'reversion_skipped_problems': reversion_skip_count if SKIP_BOOLEAN_FOR_REVERSION else 0,\n",
        "}\n",
        "\n",
        "benchmark_table = pd.DataFrame([\n",
        "    {'Metric': 'Dataset', 'Value': benchmark_summary['dataset']},\n",
        "    {'Metric': 'Problems Evaluated', 'Value': benchmark_summary['problems_evaluated']},\n",
        "    {'Metric': 'Generations per Problem', 'Value': benchmark_summary['generations_per_problem']},\n",
        "    {'Metric': 'OC pass@1', 'Value': metrics_summary['OC']},\n",
        "    {'Metric': 'OR pass@1', 'Value': metrics_summary['OR']},\n",
        "    {'Metric': 'MC pass@1', 'Value': metrics_summary['MC']},\n",
        "    {'Metric': 'MR pass@1', 'Value': metrics_summary['MR']},\n",
        "    {'Metric': 'Avg latency (s)', 'Value': benchmark_summary['avg_latency_s']},\n",
        "    {'Metric': 'Reversion skipped', 'Value': benchmark_summary['reversion_skipped_problems']},\n",
        "])\n",
        "formatters = {'Value': (lambda val: f\"{val:.3f}\" if isinstance(val, float) and val is not None else val)}\n",
        "\n",
        "print('\u2713 Execution prediction benchmark complete!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a067d6",
      "metadata": {},
      "source": [
        "## Step 7: Visualize Execution Prediction Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31382f50",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if 'metrics_summary' not in globals():\n",
        "    raise RuntimeError('Run the benchmark cell first.')\n",
        "\n",
        "metrics = {k: metrics_summary.get(k) for k in ['OC', 'OR', 'MC', 'MR']}\n",
        "labels = list(metrics.keys())\n",
        "values = [metrics[k] * 100 if metrics[k] is not None else None for k in labels]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "bars = plt.bar(labels, [v if v is not None else 0 for v in values], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for bar, val in zip(bars, values):\n",
        "    if val is None:\n",
        "        bar.set_alpha(0.3)\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), 'N/A', ha='center', va='bottom')\n",
        "    else:\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f\"{val:.1f}%\", ha='center', va='bottom')\n",
        "plt.ylim(0, 100)\n",
        "plt.ylabel('pass@1 (%)')\n",
        "plt.title('Execution Prediction Metrics (OC/OR/MC/MR)')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "display(benchmark_table.style.format(formatters))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda7d742",
      "metadata": {},
      "source": [
        "## Step 8: Save Execution Prediction Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c219bcef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "if 'benchmark_summary' not in globals():\n",
        "    raise RuntimeError('Run the benchmark cell first.')\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_filename = f\"execution_prediction_metrics_{timestamp}.json\"\n",
        "\n",
        "def _clean(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, float) and math.isnan(value):\n",
        "        return None\n",
        "    return value\n",
        "\n",
        "payload = {\n",
        "    'benchmark_summary': {k: _clean(v) for k, v in benchmark_summary.items()},\n",
        "    'metrics_summary': {k: _clean(v) for k, v in metrics_summary.items()},\n",
        "    'metrics_counts': metrics_counts,\n",
        "}\n",
        "\n",
        "with open(output_filename, 'w') as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "\n",
        "print(f'\u2713 Saved metrics to {output_filename}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0864019",
      "metadata": {},
      "source": [
        "## Step 9: Execution Choice Benchmark (Preference / Correctness / Reversion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c825f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "NUM_PROBLEMS_CHOICE = 20\n",
        "START_INDEX_CHOICE = 0\n",
        "NUM_RUNS_PER_PROBLEM = 2\n",
        "MAX_NEW_TOKENS_CHOICE = 800\n",
        "TEMPERATURE_CHOICE = TEMPERATURE\n",
        "TOP_P_CHOICE = TOP_P\n",
        "\n",
        "def _choice_pass(success: int, total: int):\n",
        "    return success / total if total else None\n",
        "\n",
        "choice_split = dataset[SPLIT]\n",
        "stop_choice = len(choice_split) if NUM_PROBLEMS_CHOICE is None else min(len(choice_split), START_INDEX_CHOICE + NUM_PROBLEMS_CHOICE)\n",
        "choice_indices = list(range(START_INDEX_CHOICE, stop_choice))\n",
        "if not choice_indices:\n",
        "    raise ValueError('No problems selected for execution choice. Adjust START_INDEX_CHOICE/NUM_PROBLEMS_CHOICE.')\n",
        "\n",
        "orderings = [True, False]\n",
        "selected_orderings = orderings[:NUM_RUNS_PER_PROBLEM]\n",
        "\n",
        "execution_choice_counts = {\n",
        "    'preference': {'original': 0, 'mutated': 0, 'total': 0},\n",
        "    'OC': {'correct': 0, 'total': 0, 'reversion_correct': 0, 'reversion_total': 0},\n",
        "    'MC': {'correct': 0, 'total': 0, 'reversion_correct': 0, 'reversion_total': 0},\n",
        "    'invalid_runs': 0,\n",
        "}\n",
        "execution_choice_results = []\n",
        "choice_latencies = []\n",
        "\n",
        "for idx in tqdm(choice_indices, desc='Execution choice'):\n",
        "    sample = choice_split[idx]\n",
        "    original_output = sample['output']\n",
        "    mutated_output = sample.get('mutated_output') or original_output\n",
        "\n",
        "    include_reversion = True\n",
        "    if SKIP_BOOLEAN_FOR_REVERSION and (is_boolean_output(original_output) or is_boolean_output(mutated_output)):\n",
        "        include_reversion = False\n",
        "\n",
        "    base_seed = SEED + idx * 1000\n",
        "\n",
        "    for run_offset, original_first in enumerate(selected_orderings):\n",
        "        prompt, mapping = build_execution_choice_prompt(sample, original_first=original_first)\n",
        "        response_text, latency = _decode_predictions(prompt, base_seed + run_offset)\n",
        "        choice_latencies.append(latency)\n",
        "\n",
        "        run_record = {\n",
        "            'problem_index': int(idx),\n",
        "            'problem_id': sample.get('id'),\n",
        "            'function_name': sample.get('function_name'),\n",
        "            'original_first': original_first,\n",
        "            'response': response_text,\n",
        "            'latency_s': latency,\n",
        "            'include_reversion': include_reversion,\n",
        "            'chosen_program_letter': None,\n",
        "            'chosen_program_type': None,\n",
        "            'assertion': None,\n",
        "            'prediction': None,\n",
        "            'correct_for_chosen_program': None,\n",
        "            'reversion_for_other_program': None,\n",
        "            'correctness_error': None,\n",
        "            'reversion_error': None,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            parsed = parse_execution_choice_response(response_text)\n",
        "        except Exception as exc:\n",
        "            execution_choice_counts['invalid_runs'] += 1\n",
        "            run_record['correctness_error'] = str(exc)\n",
        "            execution_choice_results.append(run_record)\n",
        "            continue\n",
        "\n",
        "        chosen_letter = parsed.get('chosen_program')\n",
        "        assertion_text = parsed.get('assertion', '')\n",
        "        if not chosen_letter or chosen_letter not in mapping:\n",
        "            execution_choice_counts['invalid_runs'] += 1\n",
        "            run_record['correctness_error'] = 'Missing/invalid chosen_program in response.'\n",
        "            execution_choice_results.append(run_record)\n",
        "            continue\n",
        "\n",
        "        chosen_type = mapping[chosen_letter]\n",
        "        predicted_output = extract_output_from_assertion(assertion_text)\n",
        "        chosen_output = original_output if chosen_type == 'original' else mutated_output\n",
        "        other_output = mutated_output if chosen_type == 'original' else original_output\n",
        "\n",
        "        is_correct, _ = check_predicted_output(predicted_output, chosen_output)\n",
        "        reversion_flag = None\n",
        "        if include_reversion:\n",
        "            reversion_flag, _ = check_predicted_output(predicted_output, other_output)\n",
        "\n",
        "        run_record.update({\n",
        "            'chosen_program_letter': chosen_letter,\n",
        "            'chosen_program_type': chosen_type,\n",
        "            'assertion': assertion_text,\n",
        "            'prediction': predicted_output,\n",
        "            'correct_for_chosen_program': bool(is_correct),\n",
        "            'reversion_for_other_program': bool(reversion_flag) if isinstance(reversion_flag, bool) else None,\n",
        "        })\n",
        "        execution_choice_results.append(run_record)\n",
        "\n",
        "        execution_choice_counts['preference']['total'] += 1\n",
        "        execution_choice_counts['preference'][chosen_type] += 1\n",
        "\n",
        "        metric_key = 'OC' if chosen_type == 'original' else 'MC'\n",
        "        execution_choice_counts[metric_key]['total'] += 1\n",
        "        if is_correct:\n",
        "            execution_choice_counts[metric_key]['correct'] += 1\n",
        "        if include_reversion:\n",
        "            rev_key = 'OR' if chosen_type == 'original' else 'MR'\n",
        "        else:\n",
        "            rev_key = None\n",
        "        if include_reversion:\n",
        "            target = execution_choice_counts['OC'] if chosen_type == 'original' else execution_choice_counts['MC']\n",
        "            target['reversion_total'] += 1\n",
        "            if reversion_flag:\n",
        "                target['reversion_correct'] += 1\n",
        "\n",
        "execution_choice_summary = {\n",
        "    'preference_original': execution_choice_counts['preference']['original'] / execution_choice_counts['preference']['total'] if execution_choice_counts['preference']['total'] else None,\n",
        "    'preference_mutated': execution_choice_counts['preference']['mutated'] / execution_choice_counts['preference']['total'] if execution_choice_counts['preference']['total'] else None,\n",
        "    'oc_correct': _choice_pass(execution_choice_counts['OC']['correct'], execution_choice_counts['OC']['total']),\n",
        "    'or_reversion': _choice_pass(execution_choice_counts['OC']['reversion_correct'], execution_choice_counts['OC']['reversion_total']),\n",
        "    'mc_correct': _choice_pass(execution_choice_counts['MC']['correct'], execution_choice_counts['MC']['total']),\n",
        "    'mr_reversion': _choice_pass(execution_choice_counts['MC']['reversion_correct'], execution_choice_counts['MC']['reversion_total']),\n",
        "    'invalid_runs': execution_choice_counts['invalid_runs'],\n",
        "}\n",
        "\n",
        "choice_metrics_table = pd.DataFrame([\n",
        "    {'Metric': 'Preference (Original)', 'Value': execution_choice_summary['preference_original']},\n",
        "    {'Metric': 'Preference (Mutated)', 'Value': execution_choice_summary['preference_mutated']},\n",
        "    {'Metric': 'OC Correct', 'Value': execution_choice_summary['oc_correct']},\n",
        "    {'Metric': 'OR Reversion', 'Value': execution_choice_summary['or_reversion']},\n",
        "    {'Metric': 'MC Correct', 'Value': execution_choice_summary['mc_correct']},\n",
        "    {'Metric': 'MR Reversion', 'Value': execution_choice_summary['mr_reversion']},\n",
        "])\n",
        "\n",
        "print('\u2713 Execution choice benchmark complete!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6579c003",
      "metadata": {},
      "source": [
        "## Step 10: Visualize Execution Choice Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43db393",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if 'execution_choice_summary' not in globals():\n",
        "    raise RuntimeError('Run the execution choice benchmark first.')\n",
        "\n",
        "choice_metrics = {\n",
        "    'Pref. Original': execution_choice_summary.get('preference_original'),\n",
        "    'Pref. Mutated': execution_choice_summary.get('preference_mutated'),\n",
        "    'OC Correct': execution_choice_summary.get('oc_correct'),\n",
        "    'OR Reversion': execution_choice_summary.get('or_reversion'),\n",
        "    'MC Correct': execution_choice_summary.get('mc_correct'),\n",
        "    'MR Reversion': execution_choice_summary.get('mr_reversion'),\n",
        "}\n",
        "labels = list(choice_metrics.keys())\n",
        "values = [choice_metrics[k] * 100 if choice_metrics[k] is not None else None for k in labels]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "bars = plt.bar(labels, [v if v is not None else 0 for v in values], color=['#9467bd', '#8c564b', '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for bar, val in zip(bars, values):\n",
        "    if val is None:\n",
        "        bar.set_alpha(0.3)\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), 'N/A', ha='center', va='bottom')\n",
        "    else:\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f\"{val:.1f}%\", ha='center', va='bottom')\n",
        "plt.ylim(0, 100)\n",
        "plt.ylabel('Rate (%)')\n",
        "plt.title('Execution Choice Metrics')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "display(choice_metrics_table.style.format({'Value': lambda v: f\"{v:.3f}\" if isinstance(v, float) and v is not None else v}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa421393",
      "metadata": {},
      "source": [
        "## Step 11: Save Execution Choice Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0757ef6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "if 'execution_choice_summary' not in globals():\n",
        "    raise RuntimeError('Run the execution choice benchmark first.')\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_filename = f\"execution_choice_metrics_{timestamp}.json\"\n",
        "\n",
        "def _clean(value):\n",
        "    if value is None:\n",
        "        return None\n",
        "    if isinstance(value, float) and math.isnan(value):\n",
        "        return None\n",
        "    return value\n",
        "\n",
        "payload = {\n",
        "    'execution_choice_summary': {k: _clean(v) for k, v in execution_choice_summary.items()},\n",
        "    'execution_choice_counts': execution_choice_counts,\n",
        "    'execution_choice_results': execution_choice_results,\n",
        "}\n",
        "\n",
        "with open(output_filename, 'w') as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "\n",
        "print(f'\u2713 Saved execution choice metrics to {output_filename}')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}