{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-OSS 20B Evaluation on LeetCode Dataset\n",
    "\n",
    "This notebook evaluates OpenAI's gpt-oss-20b model on the LeetCode contests dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Free Google Colab (T4 GPU)\n",
    "- HuggingFace account (to load dataset)\n",
    "\n",
    "**Author**: Code Reasoning Reproduction Team  \n",
    "**Date**: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "Install required packages for mxfp4 quantization support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install bleeding-edge PyTorch and transformers\n",
    "!pip install -q --upgrade torch\n",
    "!pip install -q transformers triton==3.4 kernels\n",
    "!pip uninstall -q torchvision torchaudio -y\n",
    "\n",
    "# Install datasets library\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **IMPORTANT**: Please restart your Colab runtime after running the cell above.\n",
    "\n",
    "Click: **Runtime → Restart runtime**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load GPT-OSS 20B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading gpt-oss-20b model...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load LeetCode Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: Replace with your HuggingFace dataset repo ID\n",
    "DATASET_REPO_ID = \"YOUR_USERNAME/leetcode-contests-431-467\"\n",
    "\n",
    "print(f\"Loading dataset from {DATASET_REPO_ID}...\")\n",
    "dataset = load_dataset(DATASET_REPO_ID)\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded!\")\n",
    "print(f\"Total samples: {len(dataset['train'])}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"  ID: {sample['id']}\")\n",
    "print(f\"  Function: {sample['function_name']}\")\n",
    "print(f\"  Difficulty: {sample['difficulty']}\")\n",
    "print(f\"  Input: {sample['input'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# BASE_IMPORTS for code execution\n",
    "BASE_IMPORTS = \"\"\"from itertools import accumulate, chain, combinations, count, cycle, permutations, product, groupby, islice, repeat, zip_longest\n",
    "from copy import deepcopy\n",
    "from string import ascii_uppercase, ascii_lowercase\n",
    "from math import floor, factorial, log, log2, log10, sqrt, prod, comb, lcm, gcd, ceil, inf, isqrt, isfinite\n",
    "from collections import defaultdict, deque, Counter, OrderedDict\n",
    "from bisect import bisect, bisect_left, bisect_right, insort\n",
    "from heapq import heappush, heappop, heapify, merge, heapreplace, heappushpop, nsmallest, nlargest\n",
    "from functools import reduce, cache, lru_cache, partial, cmp_to_key\n",
    "from random import randrange, shuffle, randint, getrandbits\n",
    "from operator import itemgetter, add, sub, mul, iand, ior, xor, and_, or_\n",
    "from re import search as re_search\n",
    "from os.path import commonprefix\n",
    "from sys import maxsize\n",
    "from typing import List, Tuple, Dict, Set, Optional, Union, Any, Callable, Iterable, Iterator, Generator, NamedTuple\n",
    "import copy, string, math, collections, bisect, heapq, functools, random, itertools, operator, re\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(sample: Dict) -> str:\n",
    "    \"\"\"Build prompt for code generation\"\"\"\n",
    "    function_name = sample['function_name']\n",
    "    \n",
    "    prompt = f\"\"\"Write a Python function to solve this LeetCode problem:\n",
    "\n",
    "Function to implement: {function_name}\n",
    "\n",
    "Requirements:\n",
    "- Implement ONLY the function, no class wrapper\n",
    "- Use efficient algorithms\n",
    "- Handle edge cases\n",
    "- Return the correct output type\n",
    "\n",
    "Write the complete function implementation:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def extract_code_from_response(response: str) -> str:\n",
    "    \"\"\"Extract Python code from model response\"\"\"\n",
    "    # Look for code blocks\n",
    "    pattern = r'```python\\n(.*?)```'\n",
    "    matches = re.findall(pattern, response, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[0].strip()\n",
    "    \n",
    "    # Look for generic code blocks\n",
    "    pattern = r'```\\n(.*?)```'\n",
    "    matches = re.findall(pattern, response, re.DOTALL)\n",
    "    if matches:\n",
    "        return matches[0].strip()\n",
    "    \n",
    "    # Return as-is\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def execute_code_with_test(code: str, test_input: str, expected_output: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Execute generated code and check correctness\"\"\"\n",
    "    try:\n",
    "        full_code = f\"{BASE_IMPORTS}\\n{code}\\nresult = {test_input}\"\n",
    "        namespace = {}\n",
    "        exec(full_code, namespace)\n",
    "        actual_output = repr(namespace.get('result'))\n",
    "        \n",
    "        is_correct = (actual_output == expected_output)\n",
    "        return is_correct, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Code for One Sample (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one sample\n",
    "test_sample = dataset['train'][0]\n",
    "\n",
    "prompt = build_prompt(test_sample)\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Generate code with gpt-oss\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Generate clean, efficient code.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    reasoning_effort=\"medium\",  # Can be \"low\", \"medium\", or \"high\"\n",
    ").to(model.device)\n",
    "\n",
    "print(\"Generating code...\")\n",
    "generated = model.generate(**inputs, max_new_tokens=500)\n",
    "response = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Extract and test code\n",
    "generated_code = extract_code_from_response(response)\n",
    "print(\"\\nExtracted Code:\")\n",
    "print(generated_code)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Test correctness\n",
    "is_correct, error = execute_code_with_test(\n",
    "    generated_code,\n",
    "    test_sample['input'],\n",
    "    test_sample['output']\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Result: {'✓ CORRECT' if is_correct else '✗ INCORRECT'}\")\n",
    "if error:\n",
    "    print(f\"Error: {error}\")\n",
    "print(f\"Expected: {test_sample['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate on Multiple Samples\n",
    "\n",
    "**Note**: Evaluating all 347 samples will take a while. Start with a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "NUM_SAMPLES = 10  # Start small, increase later\n",
    "REASONING_EFFORT = \"medium\"  # \"low\", \"medium\", or \"high\"\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "\n",
    "print(f\"Evaluating {NUM_SAMPLES} samples with reasoning_effort={REASONING_EFFORT}...\\n\")\n",
    "\n",
    "for idx in tqdm(range(NUM_SAMPLES)):\n",
    "    sample = dataset['train'][idx]\n",
    "    \n",
    "    try:\n",
    "        # Build prompt\n",
    "        prompt = build_prompt(sample)\n",
    "        \n",
    "        # Generate code\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "            reasoning_effort=REASONING_EFFORT,\n",
    "        ).to(model.device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        generated = model.generate(**inputs, max_new_tokens=500)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        response = tokenizer.decode(\n",
    "            generated[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Extract code\n",
    "        generated_code = extract_code_from_response(response)\n",
    "        \n",
    "        # Test\n",
    "        is_correct, error = execute_code_with_test(\n",
    "            generated_code,\n",
    "            sample['input'],\n",
    "            sample['output']\n",
    "        )\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"problem_id\": sample['id'],\n",
    "            \"function_name\": sample['function_name'],\n",
    "            \"difficulty\": sample['difficulty'],\n",
    "            \"correct\": is_correct,\n",
    "            \"error\": error,\n",
    "            \"latency_s\": latency,\n",
    "            \"generated_code\": generated_code\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on sample {idx}: {e}\")\n",
    "        results.append({\n",
    "            \"problem_id\": sample['id'],\n",
    "            \"function_name\": sample['function_name'],\n",
    "            \"difficulty\": sample['difficulty'],\n",
    "            \"correct\": False,\n",
    "            \"error\": str(e),\n",
    "            \"latency_s\": 0,\n",
    "            \"generated_code\": \"\"\n",
    "        })\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: gpt-oss-20b\")\n",
    "print(f\"Reasoning effort: {REASONING_EFFORT}\")\n",
    "print(f\"Total samples: {NUM_SAMPLES}\")\n",
    "print(f\"Correct: {correct_count}\")\n",
    "print(f\"pass@1: {correct_count/NUM_SAMPLES*100:.2f}%\")\n",
    "print(f\"Average latency: {sum(r['latency_s'] for r in results)/len(results):.2f}s\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Breakdown by difficulty\n",
    "from collections import defaultdict\n",
    "by_diff = defaultdict(lambda: {'total': 0, 'correct': 0})\n",
    "for r in results:\n",
    "    by_diff[r['difficulty']]['total'] += 1\n",
    "    if r['correct']:\n",
    "        by_diff[r['difficulty']]['correct'] += 1\n",
    "\n",
    "print(\"\\nBy Difficulty:\")\n",
    "for diff in ['easy', 'medium', 'hard']:\n",
    "    if diff in by_diff:\n",
    "        total = by_diff[diff]['total']\n",
    "        correct = by_diff[diff]['correct']\n",
    "        print(f\"  {diff.capitalize()}: {correct}/{total} ({correct/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save results to JSON\n",
    "output_filename = f\"gpt_oss_20b_results_{REASONING_EFFORT}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"model\": \"gpt-oss-20b\",\n",
    "    \"reasoning_effort\": REASONING_EFFORT,\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"correct_count\": correct_count,\n",
    "    \"pass_at_1\": correct_count / NUM_SAMPLES,\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_filename}\")\n",
    "\n",
    "# Download the file\n",
    "from google.colab import files\n",
    "files.download(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Reasoning Efforts (Optional)\n",
    "\n",
    "Evaluate with different reasoning efforts to see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare low vs medium vs high reasoning\n",
    "reasoning_levels = [\"low\", \"medium\", \"high\"]\n",
    "comparison_results = {}\n",
    "\n",
    "NUM_COMPARISON_SAMPLES = 5  # Use small number for comparison\n",
    "\n",
    "for reasoning_effort in reasoning_levels:\n",
    "    print(f\"\\nTesting reasoning_effort={reasoning_effort}...\")\n",
    "    correct = 0\n",
    "    total_latency = 0\n",
    "    \n",
    "    for idx in range(NUM_COMPARISON_SAMPLES):\n",
    "        sample = dataset['train'][idx]\n",
    "        \n",
    "        prompt = build_prompt(sample)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "        ).to(model.device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        generated = model.generate(**inputs, max_new_tokens=500)\n",
    "        latency = time.time() - start_time\n",
    "        total_latency += latency\n",
    "        \n",
    "        response = tokenizer.decode(\n",
    "            generated[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        generated_code = extract_code_from_response(response)\n",
    "        is_correct, _ = execute_code_with_test(\n",
    "            generated_code, sample['input'], sample['output']\n",
    "        )\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "    \n",
    "    comparison_results[reasoning_effort] = {\n",
    "        \"correct\": correct,\n",
    "        \"pass@1\": correct / NUM_COMPARISON_SAMPLES,\n",
    "        \"avg_latency\": total_latency / NUM_COMPARISON_SAMPLES\n",
    "    }\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REASONING EFFORT COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Reasoning':<12} {'pass@1':<10} {'Avg Latency':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for level, stats in comparison_results.items():\n",
    "    print(f\"{level:<12} {stats['pass@1']*100:>6.1f}%   {stats['avg_latency']:>10.2f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Increase NUM_SAMPLES** to evaluate on more problems\n",
    "2. **Try different reasoning_effort** levels\n",
    "3. **Compare with other models** (DeepSeek-R1, GPT-4o, etc.)\n",
    "4. **Analyze error patterns** to understand model weaknesses\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: To run on full dataset (347 samples), expect ~1-2 hours on free Colab."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
