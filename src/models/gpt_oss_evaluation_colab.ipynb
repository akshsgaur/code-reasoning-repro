{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GPT-OSS 20B Evaluation on LeetCode Dataset\n\nThis notebook evaluates OpenAI's gpt-oss-20b model on the LeetCode contests dataset using **Execution Prediction**.\n\n**Methodology**: Instead of generating code, the model is given a Python program and predicts its output for a given input (following the paper's approach).\n\n**Requirements**:\n- Free Google Colab (T4 GPU)\n- HuggingFace account (to load dataset)\n\n**Author**: Code Reasoning Reproduction Team  \n**Date**: 2025"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "Install required packages for mxfp4 quantization support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install bleeding-edge PyTorch and transformers\n",
    "!pip install -q --upgrade torch\n",
    "!pip install -q transformers triton==3.4 kernels\n",
    "!pip uninstall -q torchvision torchaudio -y\n",
    "\n",
    "# Install datasets library\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **IMPORTANT**: Please restart your Colab runtime after running the cell above.\n",
    "\n",
    "Click: **Runtime → Restart runtime**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load GPT-OSS 20B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading gpt-oss-20b model...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load LeetCode Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: Replace with your HuggingFace dataset repo ID\n",
    "DATASET_REPO_ID = \"YOUR_USERNAME/leetcode-contests-431-467\"\n",
    "\n",
    "print(f\"Loading dataset from {DATASET_REPO_ID}...\")\n",
    "dataset = load_dataset(DATASET_REPO_ID)\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded!\")\n",
    "print(f\"Total samples: {len(dataset['train'])}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"  ID: {sample['id']}\")\n",
    "print(f\"  Function: {sample['function_name']}\")\n",
    "print(f\"  Difficulty: {sample['difficulty']}\")\n",
    "print(f\"  Input: {sample['input'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nfrom typing import Dict, List, Tuple\n\ndef build_prompt(sample: Dict) -> str:\n    \"\"\"\n    Build prompt for execution prediction\n    Uses the exact format from the paper (Execution Prediction Prompt - Zero-Shot)\n    \n    Format:\n    You are given a Python program and an assertion containing an input to a function.\n    Replace the ?? in the assertion with a literal representing the function's return\n    value for the given input. Provide the full assertion in [ANSWER] and [/ANSWER] tags.\n    \n    [PYTHON]\n    {program}\n    assert {function_name}({input}) == ??\n    [/PYTHON]\n    \"\"\"\n    function_name = sample['function_name']\n    code = sample['code']  # Use the collected solution as the program\n    test_input = sample['input']  # e.g., \"maxLength(nums=[1,2,3])\"\n    \n    # Extract just the input arguments (remove function name and parentheses if present)\n    if test_input.startswith(f\"{function_name}(\") and test_input.endswith(\")\"):\n        input_args = test_input[len(function_name)+1:-1]\n    else:\n        input_args = test_input\n    \n    prompt = f\"\"\"You are given a Python program and an assertion containing an input to a function. Replace the ?? in the assertion with a literal (no unsimplified expressions, no function calls) representing the function's return value for the given input. Execute the program exactly as written, even if it is incorrect or incomplete. For your final answer, provide the full assertion in [ANSWER] and [/ANSWER] tags.\n\n[PYTHON]\n{code}\nassert {function_name}({input_args}) == ??\n[/PYTHON]\"\"\"\n    \n    return prompt\n\n\ndef extract_answer_from_response(response: str) -> str:\n    \"\"\"\n    Extract predicted answer from [ANSWER] tags\n    \n    Expected format: [ANSWER] assert function_name(input) == output [/ANSWER]\n    We want to extract just the output value\n    \"\"\"\n    \n    # Look for [ANSWER] tags\n    pattern = r'\\[ANSWER\\](.*?)\\[/ANSWER\\]'\n    matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)\n    if matches:\n        assertion = matches[0].strip()\n        \n        # Parse the assertion to extract the predicted value\n        # Format: \"assert function_name(input) == value\"\n        match = re.search(r'assert\\s+\\w+\\([^)]*\\)\\s*==\\s*(.+)', assertion)\n        if match:\n            predicted_value = match.group(1).strip()\n            return predicted_value\n        \n        # If we can't parse it, return the whole assertion\n        return assertion\n    \n    # Fallback: try to find \"assert ... == VALUE\" pattern anywhere\n    pattern = r'assert\\s+\\w+\\([^)]*\\)\\s*==\\s*(.+?)(?:\\n|$)'\n    matches = re.findall(pattern, response, re.MULTILINE)\n    if matches:\n        return matches[0].strip()\n    \n    # Return as-is if no answer tags found\n    return response.strip()\n\n\ndef check_predicted_output(predicted_output: str, expected_output: str) -> Tuple[bool, str]:\n    \"\"\"\n    Compare predicted output with expected output\n    \n    For execution prediction task, the model predicts what the output will be.\n    We simply compare the predicted value with the actual expected value.\n    \"\"\"\n    try:\n        # Normalize both strings for comparison\n        predicted = predicted_output.strip()\n        expected = expected_output.strip()\n        \n        # Direct string comparison\n        if predicted == expected:\n            return (True, None)\n        \n        # Try evaluating both as Python literals and compare\n        try:\n            import ast\n            predicted_val = ast.literal_eval(predicted)\n            expected_val = ast.literal_eval(expected)\n            \n            if predicted_val == expected_val:\n                return (True, None)\n        except (ValueError, SyntaxError):\n            # If we can't parse as literals, fall back to string comparison\n            pass\n        \n        # Not equal\n        return (False, f\"Predicted: {predicted}, Expected: {expected}\")\n    \n    except Exception as e:\n        return (False, str(e))\n\n\nprint(\"✓ Helper functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Test Execution Prediction on One Sample\n\nThis cell demonstrates the **Execution Prediction** task:\n1. Model receives a Python program and an assertion with `??`\n2. Model predicts what the output will be\n3. Model provides answer in `[ANSWER]...[/ANSWER]` tags"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with one sample\ntest_sample = dataset['train'][0]\n\nprompt = build_prompt(test_sample)\nprint(\"Prompt:\")\nprint(prompt)\nprint(\"\\n\" + \"=\"*60)\n\n# Generate prediction with gpt-oss\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n    reasoning_effort=\"medium\",  # Can be \"low\", \"medium\", or \"high\"\n).to(model.device)\n\nprint(\"Generating prediction...\")\n# Increase max_new_tokens to allow for reasoning + answer\ngenerated = model.generate(**inputs, max_new_tokens=1000)\nresponse = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n\nprint(\"\\nGenerated Response:\")\nprint(response)\nprint(\"\\n\" + \"=\"*60)\n\n# Extract predicted output\npredicted_output = extract_answer_from_response(response)\nprint(\"\\nExtracted Predicted Output:\")\nprint(predicted_output)\nprint(\"\\n\" + \"=\"*60)\n\n# Check correctness\nis_correct, error = check_predicted_output(\n    predicted_output,\n    test_sample['output']\n)\n\nprint(f\"\\nTest Result: {'✓ CORRECT' if is_correct else '✗ INCORRECT'}\")\nif error:\n    print(f\"Error: {error}\")\nprint(f\"Expected: {test_sample['output']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate on Multiple Samples\n",
    "\n",
    "**Note**: Evaluating all 347 samples will take a while. Start with a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom tqdm.auto import tqdm\n\n# Configuration\nNUM_SAMPLES = 10  # Start small, increase later\nREASONING_EFFORT = \"medium\"  # \"low\", \"medium\", or \"high\"\nMAX_NEW_TOKENS = 1000  # Increased to allow for reasoning + answer\n\nresults = []\ncorrect_count = 0\n\nprint(f\"Evaluating {NUM_SAMPLES} samples with reasoning_effort={REASONING_EFFORT}...\\n\")\n\nfor idx in tqdm(range(NUM_SAMPLES)):\n    sample = dataset['train'][idx]\n    \n    try:\n        # Build prompt\n        prompt = build_prompt(sample)\n        \n        # Generate prediction\n        messages = [\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        \n        inputs = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            return_tensors=\"pt\",\n            return_dict=True,\n            reasoning_effort=REASONING_EFFORT,\n        ).to(model.device)\n        \n        start_time = time.time()\n        generated = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n        latency = time.time() - start_time\n        \n        response = tokenizer.decode(\n            generated[0][inputs[\"input_ids\"].shape[-1]:],\n            skip_special_tokens=True\n        )\n        \n        # Extract predicted output\n        predicted_output = extract_answer_from_response(response)\n        \n        # Check correctness\n        is_correct, error = check_predicted_output(\n            predicted_output,\n            sample['output']\n        )\n        \n        if is_correct:\n            correct_count += 1\n        \n        results.append({\n            \"problem_id\": sample['id'],\n            \"function_name\": sample['function_name'],\n            \"difficulty\": sample['difficulty'],\n            \"correct\": is_correct,\n            \"error\": error,\n            \"latency_s\": latency,\n            \"predicted_output\": predicted_output,\n            \"expected_output\": sample['output']\n        })\n        \n    except Exception as e:\n        print(f\"\\nError on sample {idx}: {e}\")\n        results.append({\n            \"problem_id\": sample['id'],\n            \"function_name\": sample['function_name'],\n            \"difficulty\": sample['difficulty'],\n            \"correct\": False,\n            \"error\": str(e),\n            \"latency_s\": 0,\n            \"predicted_output\": \"\",\n            \"expected_output\": sample['output']\n        })\n\n# Print results\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*60)\nprint(f\"Model: gpt-oss-20b\")\nprint(f\"Reasoning effort: {REASONING_EFFORT}\")\nprint(f\"Total samples: {NUM_SAMPLES}\")\nprint(f\"Correct: {correct_count}\")\nprint(f\"pass@1: {correct_count/NUM_SAMPLES*100:.2f}%\")\nprint(f\"Average latency: {sum(r['latency_s'] for r in results)/len(results):.2f}s\")\nprint(\"=\"*60)\n\n# Breakdown by difficulty\nfrom collections import defaultdict\nby_diff = defaultdict(lambda: {'total': 0, 'correct': 0})\nfor r in results:\n    by_diff[r['difficulty']]['total'] += 1\n    if r['correct']:\n        by_diff[r['difficulty']]['correct'] += 1\n\nprint(\"\\nBy Difficulty:\")\nfor diff in ['easy', 'medium', 'hard']:\n    if diff in by_diff:\n        total = by_diff[diff]['total']\n        correct = by_diff[diff]['correct']\n        print(f\"  {diff.capitalize()}: {correct}/{total} ({correct/total*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Save results to JSON\n",
    "output_filename = f\"gpt_oss_20b_results_{REASONING_EFFORT}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"model\": \"gpt-oss-20b\",\n",
    "    \"reasoning_effort\": REASONING_EFFORT,\n",
    "    \"num_samples\": NUM_SAMPLES,\n",
    "    \"correct_count\": correct_count,\n",
    "    \"pass_at_1\": correct_count / NUM_SAMPLES,\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_filename}\")\n",
    "\n",
    "# Download the file\n",
    "from google.colab import files\n",
    "files.download(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compare Reasoning Efforts (Optional)\n",
    "\n",
    "Evaluate with different reasoning efforts to see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare low vs medium vs high reasoning\nreasoning_levels = [\"low\", \"medium\", \"high\"]\ncomparison_results = {}\n\nNUM_COMPARISON_SAMPLES = 5  # Use small number for comparison\nMAX_NEW_TOKENS = 1000  # Allow enough tokens for reasoning + answer\n\nfor reasoning_effort in reasoning_levels:\n    print(f\"\\nTesting reasoning_effort={reasoning_effort}...\")\n    correct = 0\n    total_latency = 0\n    \n    for idx in range(NUM_COMPARISON_SAMPLES):\n        sample = dataset['train'][idx]\n        \n        prompt = build_prompt(sample)\n        messages = [\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        \n        inputs = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            return_tensors=\"pt\",\n            return_dict=True,\n            reasoning_effort=reasoning_effort,\n        ).to(model.device)\n        \n        start_time = time.time()\n        generated = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n        latency = time.time() - start_time\n        total_latency += latency\n        \n        response = tokenizer.decode(\n            generated[0][inputs[\"input_ids\"].shape[-1]:],\n            skip_special_tokens=True\n        )\n        \n        predicted_output = extract_answer_from_response(response)\n        is_correct, _ = check_predicted_output(\n            predicted_output, sample['output']\n        )\n        \n        if is_correct:\n            correct += 1\n    \n    comparison_results[reasoning_effort] = {\n        \"correct\": correct,\n        \"pass@1\": correct / NUM_COMPARISON_SAMPLES,\n        \"avg_latency\": total_latency / NUM_COMPARISON_SAMPLES\n    }\n\n# Print comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"REASONING EFFORT COMPARISON\")\nprint(\"=\"*60)\nprint(f\"{'Reasoning':<12} {'pass@1':<10} {'Avg Latency':<15}\")\nprint(\"-\" * 60)\nfor level, stats in comparison_results.items():\n    print(f\"{level:<12} {stats['pass@1']*100:>6.1f}%   {stats['avg_latency']:>10.2f}s\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Increase NUM_SAMPLES** to evaluate on more problems\n",
    "2. **Try different reasoning_effort** levels\n",
    "3. **Compare with other models** (DeepSeek-R1, GPT-4o, etc.)\n",
    "4. **Analyze error patterns** to understand model weaknesses\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: To run on full dataset (347 samples), expect ~1-2 hours on free Colab."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}