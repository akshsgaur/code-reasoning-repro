{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GPT-OSS 20B Evaluation on LeetCode Dataset\n\nThis notebook evaluates OpenAI's gpt-oss-20b model on the LeetCode contests dataset using **Execution Prediction**.\n\n**Methodology**: Instead of generating code, the model is given a Python program and predicts its output for a given input (following the paper's approach).\n\n**Requirements**:\n- Free Google Colab (T4 GPU)\n- HuggingFace account (to load dataset)\n\n**Author**: Code Reasoning Reproduction Team  \n**Date**: 2025"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "Install required packages for mxfp4 quantization support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install bleeding-edge PyTorch and transformers\n",
    "!pip install -q --upgrade torch\n",
    "!pip install -q transformers triton==3.4 kernels\n",
    "!pip uninstall -q torchvision torchaudio -y\n",
    "\n",
    "# Install datasets library\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\u26a0\ufe0f **IMPORTANT**: Please restart your Colab runtime after running the cell above.\n",
    "\n",
    "Click: **Runtime \u2192 Restart runtime**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load GPT-OSS 20B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading gpt-oss-20b model...\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "print(\"\u2713 Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load LeetCode Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: Replace with your HuggingFace dataset repo ID\n",
    "DATASET_REPO_ID = \"YOUR_USERNAME/leetcode-contests-431-467\"\n",
    "\n",
    "print(f\"Loading dataset from {DATASET_REPO_ID}...\")\n",
    "dataset = load_dataset(DATASET_REPO_ID)\n",
    "\n",
    "print(f\"\\n\u2713 Dataset loaded!\")\n",
    "print(f\"Total samples: {len(dataset['train'])}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"  ID: {sample['id']}\")\n",
    "print(f\"  Function: {sample['function_name']}\")\n",
    "print(f\"  Difficulty: {sample['difficulty']}\")\n",
    "print(f\"  Input: {sample['input'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "def build_execution_prediction_prompt(sample: Dict, use_mutated: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Build execution prediction prompt for either the original or mutated program.\n",
    "    \"\"\"\n",
    "    function_name = sample['function_name']\n",
    "\n",
    "    if use_mutated:\n",
    "        program = sample.get('mutated_code')\n",
    "        if program is None:\n",
    "            raise KeyError(\"Sample missing 'mutated_code'. Did you load the mutated dataset?\")\n",
    "        if not str(program).strip():\n",
    "            raise ValueError(\"Sample has empty 'mutated_code'.\")\n",
    "    else:\n",
    "        program = sample['code']\n",
    "\n",
    "    test_input = sample['input']\n",
    "\n",
    "    if test_input and test_input.startswith(f\"{function_name}(\") and test_input.endswith(\")\"):\n",
    "        input_args = test_input[len(function_name) + 1:-1]\n",
    "    else:\n",
    "        input_args = test_input\n",
    "\n",
    "    prompt = f\"\"\"You are given a Python program and an assertion containing an input to a function. Replace the ?? in the assertion with a literal (no unsimplified expressions, no function calls) representing the function's return value for the given input. Execute the program exactly as written, even if it is incorrect or incomplete. For your final answer, provide the full assertion in [ANSWER] and [/ANSWER] tags.\n",
    "\n",
    "[PYTHON]\n",
    "{program}\n",
    "assert {function_name}({input_args}) == ??\n",
    "[/PYTHON]\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_execution_choice_prompt(sample: Dict, original_first: bool = True) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Build execution choice prompt and mapping between program letters and program types.\n",
    "    Returns the prompt string and a mapping like {'A': 'original', 'B': 'mutated'}.\n",
    "    \"\"\"\n",
    "    function_name = sample['function_name']\n",
    "    original_code = sample['code']\n",
    "    mutated_code = sample.get('mutated_code') or original_code\n",
    "    test_input = sample['input']\n",
    "\n",
    "    if test_input and test_input.startswith(f\"{function_name}(\") and test_input.endswith(\")\"):\n",
    "        input_args = test_input[len(function_name) + 1:-1]\n",
    "    else:\n",
    "        input_args = test_input\n",
    "\n",
    "    if original_first:\n",
    "        program_a, program_b = original_code, mutated_code\n",
    "        mapping = {'A': 'original', 'B': 'mutated'}\n",
    "    else:\n",
    "        program_a, program_b = mutated_code, original_code\n",
    "        mapping = {'A': 'mutated', 'B': 'original'}\n",
    "\n",
    "    prompt_template = (\n",
    "        \"You are given two Python programs below and an assertion containing an input to a function. \"\n",
    "        \"First, choose either program, whichever one you are more confident in reasoning about. \"\n",
    "        \"Then, replace the ?? in the assertion with a literal (no unsimplified expressions, no function calls) \"\n",
    "        \"representing the function's return value for the given input on your chosen program. Execute the program \"\n",
    "        \"exactly as written, even if it is incorrect or incomplete. For your final answer, output the letter of your \"\n",
    "        \"chosen program (A or B) and the full assertion in the following json format:\n",
    "\n",
    "\"\n",
    "        \"{{\n",
    "\"\n",
    "        '  \"chosen_program\": \"A or B\",\n",
    "'\n",
    "        '  \"assertion\": \"full_assertion\"\n",
    "'\n",
    "        \"}}\n",
    "\n",
    "\"\n",
    "        \"[PROGRAM_A]\n",
    "{program_a}\n",
    "[/PROGRAM_A]\n",
    "\"\n",
    "        \"[PROGRAM_B]\n",
    "{program_b}\n",
    "[/PROGRAM_B]\n",
    "\"\n",
    "        \"[ASSERTION]\n",
    "assert {function_name}({input_args}) == ??\n",
    "[/ASSERTION]\"\n",
    "    )\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        program_a=program_a,\n",
    "        program_b=program_b,\n",
    "        function_name=function_name,\n",
    "        input_args=input_args,\n",
    "    )\n",
    "\n",
    "    return prompt, mapping\n",
    "\n",
    "\n",
    "def parse_execution_choice_response(response: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the model response for execution choice into a structured dictionary.\n",
    "    Expects a JSON blob containing \"chosen_program\" and \"assertion\" keys.\n",
    "    \"\"\"\n",
    "    json_match = re.search(r'\\{\\s*\"chosen_program\"\\s*:.*?\\}', response, re.DOTALL)\n",
    "    if not json_match:\n",
    "        raise ValueError('Could not find JSON payload in the response.')\n",
    "\n",
    "    json_text = json_match.group(0)\n",
    "    try:\n",
    "        return json.loads(json_text)\n",
    "    except json.JSONDecodeError:\n",
    "        chosen_match = re.search(r'\"chosen_program\"\\s*:\\s*\"?([A-Za-z])\"?', json_text)\n",
    "        assertion_match = re.search(r'\"assertion\"\\s*:\\s*(\"(?:[^\"\\]|\\.)*\")', json_text)\n",
    "        if not chosen_match or not assertion_match:\n",
    "            raise ValueError('Failed to parse execution choice JSON response.')\n",
    "\n",
    "        chosen_program = chosen_match.group(1)\n",
    "        assertion_literal = assertion_match.group(1)\n",
    "        try:\n",
    "            assertion = ast.literal_eval(assertion_literal)\n",
    "        except Exception:\n",
    "            assertion = assertion_literal.strip('\"')\n",
    "\n",
    "        return {\n",
    "            'chosen_program': chosen_program,\n",
    "            'assertion': assertion,\n",
    "        }\n",
    "\n",
    "\n",
    "def extract_output_from_assertion(assertion: str) -> str:\n",
    "    \"\"\"Extract the predicted output value from an assertion string.\"\"\"\n",
    "    if not assertion:\n",
    "        return ''\n",
    "\n",
    "    text = assertion.strip()\n",
    "    text = re.sub(r'^\\[ASSERTION\\]\\s*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s*\\[/ASSERTION\\]$', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    match = re.search(r'assert\\s+[\\w\\.]+\\([^)]*\\)\\s*==\\s*(.+)', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_answer_from_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract predicted answer from [ANSWER] tags.\n",
    "\n",
    "    Expected format: [ANSWER] assert function_name(input) == output [/ANSWER]\n",
    "    \"\"\"\n",
    "    pattern = r'\\[ANSWER\\](.*?)\\[/ANSWER\\]'\n",
    "    matches = re.findall(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        assertion = matches[0].strip()\n",
    "        match = re.search(r'assert\\s+\\w+\\([^)]*\\)\\s*==\\s*(.+)', assertion)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return assertion\n",
    "\n",
    "    pattern = r'assert\\s+\\w+\\([^)]*\\)\\s*==\\s*(.+?)(?:\n",
    "|$)'\n",
    "    matches = re.findall(pattern, response, re.MULTILINE)\n",
    "    if matches:\n",
    "        return matches[0].strip()\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def check_predicted_output(predicted_output: str, expected_output: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Compare predicted output with expected output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predicted = (predicted_output or '').strip()\n",
    "        expected = (expected_output or '').strip()\n",
    "\n",
    "        if predicted == expected:\n",
    "            return True, None\n",
    "\n",
    "        try:\n",
    "            predicted_val = ast.literal_eval(predicted)\n",
    "            expected_val = ast.literal_eval(expected)\n",
    "            if predicted_val == expected_val:\n",
    "                return True, None\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "        return False, f\"Predicted: {predicted}, Expected: {expected}\"\n",
    "    except Exception as exc:\n",
    "        return False, str(exc)\n",
    "\n",
    "\n",
    "def is_boolean_output(value: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether a serialized output represents a Boolean value.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        parsed = ast.literal_eval(value.strip())\n",
    "        return isinstance(parsed, bool)\n",
    "    except (ValueError, SyntaxError, AttributeError):\n",
    "        lowered = value.strip().lower()\n",
    "        return lowered in {'true', 'false'}\n",
    "\n",
    "\n",
    "print('\u2713 Helper functions defined (execution prediction + metrics)')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Test Execution Prediction on One Sample\n\nThis cell demonstrates the **Execution Prediction** task:\n1. Model receives a Python program and an assertion with `??`\n2. Model predicts what the output will be\n3. Model provides answer in `[ANSWER]...[/ANSWER]` tags"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with one sample\n",
    "test_sample = dataset['train'][0]\n",
    "\n",
    "prompt = build_execution_prediction_prompt(test_sample)\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\n",
    "\" + \"=\"*60)\n",
    "\n",
    "# Generate prediction with gpt-oss\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    reasoning_effort=\"medium\",  # Can be \"low\", \"medium\", or \"high\"\n",
    ").to(model.device)\n",
    "\n",
    "print(\"Generating prediction...\")\n",
    "# Increase max_new_tokens to allow for reasoning + answer\n",
    "generated = model.generate(**inputs, max_new_tokens=1000)\n",
    "response = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"\n",
    "Generated Response:\")\n",
    "print(response)\n",
    "print(\"\n",
    "\" + \"=\"*60)\n",
    "\n",
    "# Extract predicted output\n",
    "predicted_output = extract_answer_from_response(response)\n",
    "print(\"\n",
    "Extracted Predicted Output:\")\n",
    "print(predicted_output)\n",
    "print(\"\n",
    "\" + \"=\"*60)\n",
    "\n",
    "# Check correctness\n",
    "is_correct, error = check_predicted_output(\n",
    "    predicted_output,\n",
    "    test_sample['output']\n",
    ")\n",
    "\n",
    "print(f\"\n",
    "Test Result: {'\u2713 CORRECT' if is_correct else '\u2717 INCORRECT'}\")\n",
    "if error:\n",
    "    print(f\"Error: {error}\")\n",
    "print(f\"Expected: {test_sample['output']}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Benchmark Execution Prediction (OC / OR / MC / MR)\n",
    "\n",
    "Generate multiple samples per problem to estimate pass@1 for original/mutated correctness and reversion metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Evaluation configuration\n",
    "NUM_PROBLEMS = 20            # Set to None to evaluate the full dataset\n",
    "START_INDEX = 0\n",
    "NUM_GENERATIONS = 5\n",
    "REASONING_EFFORT = \"medium\"\n",
    "MAX_NEW_TOKENS = 1000\n",
    "TEMPERATURE = 0.6\n",
    "TOP_P = 0.95\n",
    "SEED = 42\n",
    "SKIP_BOOLEAN_FOR_REVERSION = True\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "pad_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "def generate_prediction(prompt: str, seed: int) -> dict:\n",
    "    generator = torch.Generator(device=model.device)\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        reasoning_effort=REASONING_EFFORT,\n",
    "    ).to(model.device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"generator\": generator,\n",
    "    }\n",
    "    if pad_token_id is not None:\n",
    "        generation_kwargs[\"pad_token_id\"] = pad_token_id\n",
    "\n",
    "    output_ids = model.generate(**inputs, **generation_kwargs)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    prediction = extract_answer_from_response(response)\n",
    "\n",
    "    return {\n",
    "        \"prediction\": prediction,\n",
    "        \"response\": response,\n",
    "        \"latency_s\": latency,\n",
    "    }\n",
    "\n",
    "\n",
    "if NUM_PROBLEMS is None:\n",
    "    problem_indices = list(range(len(dataset[\"train\"])))\n",
    "else:\n",
    "    stop = min(len(dataset[\"train\"]), START_INDEX + NUM_PROBLEMS)\n",
    "    problem_indices = list(range(START_INDEX, stop))\n",
    "\n",
    "if not problem_indices:\n",
    "    raise ValueError(\"No problems selected for evaluation. Adjust START_INDEX/NUM_PROBLEMS.\")\n",
    "\n",
    "print(f\"Evaluating {len(problem_indices)} LeetCode problems with {NUM_GENERATIONS} generations each.\")\n",
    "print(f\"Reasoning effort: {REASONING_EFFORT}, temperature: {TEMPERATURE}, top_p: {TOP_P}\n",
    "\")\n",
    "\n",
    "metrics_counts = {\n",
    "    \"OC\": {\"success\": 0, \"total\": 0},\n",
    "    \"OR\": {\"success\": 0, \"total\": 0},\n",
    "    \"MC\": {\"success\": 0, \"total\": 0},\n",
    "    \"MR\": {\"success\": 0, \"total\": 0},\n",
    "}\n",
    "\n",
    "reversion_skip_count = 0\n",
    "all_latencies = []\n",
    "results = []\n",
    "\n",
    "for idx in tqdm(problem_indices, desc=\"Evaluating problems\"):\n",
    "    sample = dataset[\"train\"][idx]\n",
    "\n",
    "    original_output = sample[\"output\"]\n",
    "    mutated_output = sample.get(\"mutated_output\") or original_output\n",
    "    has_mutation = sample.get(\"has_mutation\", mutated_output != original_output)\n",
    "\n",
    "    original_prompt = build_execution_prediction_prompt(sample, use_mutated=False)\n",
    "    mutated_prompt = build_execution_prediction_prompt(sample, use_mutated=True)\n",
    "\n",
    "    include_reversion = True\n",
    "    if SKIP_BOOLEAN_FOR_REVERSION and (is_boolean_output(original_output) or is_boolean_output(mutated_output)):\n",
    "        include_reversion = False\n",
    "        reversion_skip_count += 1\n",
    "\n",
    "    oc_successes = 0\n",
    "    or_successes = 0\n",
    "    mc_successes = 0\n",
    "    mr_successes = 0\n",
    "\n",
    "    original_predictions = []\n",
    "    mutated_predictions = []\n",
    "\n",
    "    seed_base = SEED + idx * 1000\n",
    "\n",
    "    for gen_idx in range(NUM_GENERATIONS):\n",
    "        pred_original = generate_prediction(original_prompt, seed_base + gen_idx)\n",
    "        original_predictions.append(pred_original)\n",
    "        all_latencies.append(pred_original[\"latency_s\"])\n",
    "\n",
    "        is_correct, _ = check_predicted_output(pred_original[\"prediction\"], original_output)\n",
    "        if is_correct:\n",
    "            oc_successes += 1\n",
    "\n",
    "        if include_reversion:\n",
    "            is_reversion, _ = check_predicted_output(pred_original[\"prediction\"], mutated_output)\n",
    "            if is_reversion:\n",
    "                or_successes += 1\n",
    "\n",
    "        pred_mutated = generate_prediction(mutated_prompt, seed_base + 500 + gen_idx)\n",
    "        mutated_predictions.append(pred_mutated)\n",
    "        all_latencies.append(pred_mutated[\"latency_s\"])\n",
    "\n",
    "        is_mutated_correct, _ = check_predicted_output(pred_mutated[\"prediction\"], mutated_output)\n",
    "        if is_mutated_correct:\n",
    "            mc_successes += 1\n",
    "\n",
    "        if include_reversion:\n",
    "            is_mutated_reversion, _ = check_predicted_output(pred_mutated[\"prediction\"], original_output)\n",
    "            if is_mutated_reversion:\n",
    "                mr_successes += 1\n",
    "\n",
    "    metrics_counts[\"OC\"][\"success\"] += oc_successes\n",
    "    metrics_counts[\"OC\"][\"total\"] += NUM_GENERATIONS\n",
    "\n",
    "    metrics_counts[\"MC\"][\"success\"] += mc_successes\n",
    "    metrics_counts[\"MC\"][\"total\"] += NUM_GENERATIONS\n",
    "\n",
    "    if include_reversion:\n",
    "        metrics_counts[\"OR\"][\"success\"] += or_successes\n",
    "        metrics_counts[\"OR\"][\"total\"] += NUM_GENERATIONS\n",
    "\n",
    "        metrics_counts[\"MR\"][\"success\"] += mr_successes\n",
    "        metrics_counts[\"MR\"][\"total\"] += NUM_GENERATIONS\n",
    "\n",
    "    results.append({\n",
    "        \"problem_index\": int(idx),\n",
    "        \"problem_id\": sample[\"id\"],\n",
    "        \"function_name\": sample[\"function_name\"],\n",
    "        \"difficulty\": sample.get(\"difficulty\"),\n",
    "        \"has_mutation\": has_mutation,\n",
    "        \"include_reversion\": include_reversion,\n",
    "        \"original_output\": original_output,\n",
    "        \"mutated_output\": mutated_output,\n",
    "        \"oc_successes\": oc_successes,\n",
    "        \"or_successes\": or_successes if include_reversion else None,\n",
    "        \"mc_successes\": mc_successes,\n",
    "        \"mr_successes\": mr_successes if include_reversion else None,\n",
    "        \"original_predictions\": original_predictions,\n",
    "        \"mutated_predictions\": mutated_predictions,\n",
    "    })\n",
    "\n",
    "\n",
    "def compute_pass(counts: dict) -> Optional[float]:\n",
    "    total = counts[\"total\"]\n",
    "    if total == 0:\n",
    "        return None\n",
    "    return counts[\"success\"] / total\n",
    "\n",
    "\n",
    "metrics_summary = {metric: compute_pass(counts) for metric, counts in metrics_counts.items()}\n",
    "\n",
    "benchmark_summary = {\n",
    "    \"dataset\": \"LeetCode\",\n",
    "    \"problems_evaluated\": len(problem_indices),\n",
    "    \"generations_per_problem\": NUM_GENERATIONS,\n",
    "    \"oc_pass_at_1\": metrics_summary[\"OC\"],\n",
    "    \"or_pass_at_1\": metrics_summary[\"OR\"],\n",
    "    \"mc_pass_at_1\": metrics_summary[\"MC\"],\n",
    "    \"mr_pass_at_1\": metrics_summary[\"MR\"],\n",
    "    \"avg_latency_s\": (sum(all_latencies) / len(all_latencies)) if all_latencies else None,\n",
    "    \"reversion_skipped_problems\": reversion_skip_count if SKIP_BOOLEAN_FOR_REVERSION else 0,\n",
    "}\n",
    "\n",
    "benchmark_table = pd.DataFrame([{\n",
    "    \"Dataset\": \"LeetCode\",\n",
    "    \"Problems Evaluated\": benchmark_summary[\"problems_evaluated\"],\n",
    "    \"Generations per Problem\": benchmark_summary[\"generations_per_problem\"],\n",
    "    \"OC pass@1\": benchmark_summary[\"oc_pass_at_1\"],\n",
    "    \"OR pass@1\": benchmark_summary[\"or_pass_at_1\"],\n",
    "    \"MC pass@1\": benchmark_summary[\"mc_pass_at_1\"],\n",
    "    \"MR pass@1\": benchmark_summary[\"mr_pass_at_1\"],\n",
    "}])\n",
    "\n",
    "formatters = {\n",
    "    \"OC pass@1\": lambda v, _pd=pd: \"N/A\" if _pd.isna(v) else f\"{v:.2%}\",\n",
    "    \"OR pass@1\": lambda v, _pd=pd: \"N/A\" if _pd.isna(v) else f\"{v:.2%}\",\n",
    "    \"MC pass@1\": lambda v, _pd=pd: \"N/A\" if _pd.isna(v) else f\"{v:.2%}\",\n",
    "    \"MR pass@1\": lambda v, _pd=pd: \"N/A\" if _pd.isna(v) else f\"{v:.2%}\",\n",
    "}\n",
    "\n",
    "display(benchmark_table.style.format(formatters))\n",
    "\n",
    "print(\"\n",
    "Counts (success / total generations):\")\n",
    "for metric, counts in metrics_counts.items():\n",
    "    print(f\"  {metric}: {counts['success']} / {counts['total']}\")\n",
    "if benchmark_summary['avg_latency_s'] is not None:\n",
    "    print(f\"\n",
    "Average latency per generation: {benchmark_summary['avg_latency_s']:.2f}s\")\n",
    "else:\n",
    "    print(\"\n",
    "Average latency per generation: N/A\")\n",
    "if SKIP_BOOLEAN_FOR_REVERSION:\n",
    "    print(f\"Problems skipped for reversion metrics (boolean outputs): {reversion_skip_count}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "if 'benchmark_summary' not in globals():\n",
    "    raise RuntimeError('Run the benchmark cell first to produce metrics.')\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"gpt_oss_20b_leetcode_benchmark_{timestamp}.json\"\n",
    "\n",
    "\n",
    "def _clean_nan(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, float) and math.isnan(value):\n",
    "        return None\n",
    "    return value\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-oss-20b\",\n",
    "    \"dataset\": benchmark_summary[\"dataset\"],\n",
    "    \"reasoning_effort\": REASONING_EFFORT,\n",
    "    \"num_problems\": benchmark_summary[\"problems_evaluated\"],\n",
    "    \"num_generations\": benchmark_summary[\"generations_per_problem\"],\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"top_p\": TOP_P,\n",
    "    \"seed\": SEED,\n",
    "    \"skip_boolean_for_reversion\": SKIP_BOOLEAN_FOR_REVERSION,\n",
    "    \"reversion_skipped_problems\": benchmark_summary[\"reversion_skipped_problems\"],\n",
    "    \"metrics\": {k: _clean_nan(v) for k, v in metrics_summary.items()},\n",
    "    \"metrics_counts\": metrics_counts,\n",
    "    \"benchmark_summary\": {k: _clean_nan(v) for k, v in benchmark_summary.items()},\n",
    "    \"results\": results,\n",
    "}\n",
    "\n",
    "if 'execution_choice_summary' in globals():\n",
    "    payload['execution_choice_summary'] = {k: _clean_nan(v) for k, v in execution_choice_summary.items()}\n",
    "    payload['execution_choice_counts'] = execution_choice_counts\n",
    "    payload['execution_choice_results'] = execution_choice_results\n",
    "    payload['execution_choice_config'] = {\n",
    "        'num_problems': NUM_PROBLEMS_CHOICE,\n",
    "        'start_index': START_INDEX_CHOICE,\n",
    "        'runs_per_problem': NUM_RUNS_PER_PROBLEM,\n",
    "        'reasoning_effort': REASONING_EFFORT_CHOICE,\n",
    "        'max_new_tokens': MAX_NEW_TOKENS_CHOICE,\n",
    "        'temperature': TEMPERATURE_CHOICE,\n",
    "        'top_p': TOP_P_CHOICE,\n",
    "        'seed': SEED_CHOICE,\n",
    "        'skip_boolean_for_reversion': SKIP_BOOLEAN_FOR_REVERSION_CHOICE,\n",
    "    }\n",
    "\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(payload, f, indent=2)\n",
    "\n",
    "print(f\"\u2713 Saved evaluation summary to {output_filename}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_filename)\n",
    "except ImportError:\n",
    "    print(\"(Optional) Run inside Colab to download the file automatically.\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Execution Choice Benchmark (Preference / Correctness / Reversion)\n",
    "\n",
    "Run the paired-program experiment to measure program preference, correctness, and reversion with order swapping."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "import time\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Execution choice configuration\n",
    "NUM_PROBLEMS_CHOICE = 20          # Set to None to evaluate the full dataset\n",
    "START_INDEX_CHOICE = 0\n",
    "NUM_RUNS_PER_PROBLEM = 2          # Two runs per problem with swapped ordering\n",
    "REASONING_EFFORT_CHOICE = \"medium\"\n",
    "MAX_NEW_TOKENS_CHOICE = 1000\n",
    "TEMPERATURE_CHOICE = 0.6\n",
    "TOP_P_CHOICE = 0.95\n",
    "SEED_CHOICE = 123\n",
    "SKIP_BOOLEAN_FOR_REVERSION_CHOICE = True\n",
    "\n",
    "if NUM_RUNS_PER_PROBLEM not in (1, 2):\n",
    "    raise ValueError('NUM_RUNS_PER_PROBLEM must be 1 or 2 for the ordering swap protocol.')\n",
    "\n",
    "torch.manual_seed(SEED_CHOICE)\n",
    "random.seed(SEED_CHOICE)\n",
    "\n",
    "\n",
    "def generate_choice_response(prompt: str, seed: int) -> dict:\n",
    "    \"\"\"Generate a model response for the execution choice prompt.\"\"\"\n",
    "    generator = torch.Generator(device=model.device)\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        reasoning_effort=REASONING_EFFORT_CHOICE,\n",
    "    ).to(model.device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS_CHOICE,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": TEMPERATURE_CHOICE,\n",
    "        \"top_p\": TOP_P_CHOICE,\n",
    "        \"generator\": generator,\n",
    "    }\n",
    "\n",
    "    pad_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "    if pad_id is not None:\n",
    "        generation_kwargs[\"pad_token_id\"] = pad_id\n",
    "\n",
    "    output_ids = model.generate(**inputs, **generation_kwargs)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"latency_s\": latency,\n",
    "    }\n",
    "\n",
    "\n",
    "if NUM_PROBLEMS_CHOICE is None:\n",
    "    problem_indices_choice = list(range(len(dataset[\"train\"])))\n",
    "else:\n",
    "    stop_choice = min(len(dataset[\"train\"]), START_INDEX_CHOICE + NUM_PROBLEMS_CHOICE)\n",
    "    problem_indices_choice = list(range(START_INDEX_CHOICE, stop_choice))\n",
    "\n",
    "if not problem_indices_choice:\n",
    "    raise ValueError('No problems selected. Adjust START_INDEX_CHOICE / NUM_PROBLEMS_CHOICE.')\n",
    "\n",
    "print(f\"Evaluating execution choice on {len(problem_indices_choice)} problems with {NUM_RUNS_PER_PROBLEM} run(s) each.\")\n",
    "print(f\"Reasoning effort: {REASONING_EFFORT_CHOICE}, temperature: {TEMPERATURE_CHOICE}, top_p: {TOP_P_CHOICE}\n",
    "\")\n",
    "\n",
    "execution_choice_counts = {\n",
    "    'preference': {'original': 0, 'mutated': 0, 'total': 0},\n",
    "    'OC': {'correct': 0, 'total': 0, 'reversion_correct': 0, 'reversion_total': 0},\n",
    "    'MC': {'correct': 0, 'total': 0, 'reversion_correct': 0, 'reversion_total': 0},\n",
    "    'invalid_runs': 0,\n",
    "}\n",
    "\n",
    "execution_choice_results = []\n",
    "execution_choice_latencies = []\n",
    "reversion_skip_count_choice = 0\n",
    "\n",
    "orderings = [True, False]  # True => original program first, False => mutated first\n",
    "selected_orderings = orderings[:NUM_RUNS_PER_PROBLEM]\n",
    "\n",
    "for idx in tqdm(problem_indices_choice, desc='Execution choice'):\n",
    "    sample = dataset['train'][idx]\n",
    "    original_output = sample['output']\n",
    "    mutated_output = sample.get('mutated_output') or original_output\n",
    "\n",
    "    include_reversion = True\n",
    "    if SKIP_BOOLEAN_FOR_REVERSION_CHOICE and (\n",
    "        is_boolean_output(original_output) or is_boolean_output(mutated_output)\n",
    "    ):\n",
    "        include_reversion = False\n",
    "        reversion_skip_count_choice += 1\n",
    "\n",
    "    base_seed = SEED_CHOICE + idx * 1000\n",
    "\n",
    "    for run_offset, original_first in enumerate(selected_orderings):\n",
    "        prompt, mapping = build_execution_choice_prompt(sample, original_first=original_first)\n",
    "        generation = generate_choice_response(prompt, base_seed + run_offset)\n",
    "        execution_choice_latencies.append(generation['latency_s'])\n",
    "\n",
    "        run_record = {\n",
    "            'problem_index': int(idx),\n",
    "            'problem_id': sample['id'],\n",
    "            'function_name': sample['function_name'],\n",
    "            'run_index': run_offset,\n",
    "            'original_first': original_first,\n",
    "            'response': generation['response'],\n",
    "            'latency_s': generation['latency_s'],\n",
    "            'include_reversion': include_reversion,\n",
    "            'chosen_program_letter': None,\n",
    "            'chosen_program_type': None,\n",
    "            'prediction': None,\n",
    "            'correct_for_chosen_program': None,\n",
    "            'reversion_for_other_program': None,\n",
    "            'error': None,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            parsed = parse_execution_choice_response(generation['response'])\n",
    "        except ValueError as exc:\n",
    "            run_record['error'] = str(exc)\n",
    "            execution_choice_counts['invalid_runs'] += 1\n",
    "            execution_choice_results.append(run_record)\n",
    "            continue\n",
    "\n",
    "        chosen_letter = str(parsed.get('chosen_program', '')).strip().upper()\n",
    "        assertion_text = parsed.get('assertion', '')\n",
    "        chosen_type = mapping.get(chosen_letter)\n",
    "\n",
    "        if chosen_type not in {'original', 'mutated'}:\n",
    "            run_record['error'] = f\"Unrecognized chosen program letter: {chosen_letter}\"\n",
    "            execution_choice_counts['invalid_runs'] += 1\n",
    "            execution_choice_results.append(run_record)\n",
    "            continue\n",
    "\n",
    "        predicted_output = extract_output_from_assertion(assertion_text)\n",
    "        chosen_output = original_output if chosen_type == 'original' else mutated_output\n",
    "        other_output = mutated_output if chosen_type == 'original' else original_output\n",
    "\n",
    "        is_correct, correctness_error = check_predicted_output(predicted_output, chosen_output)\n",
    "        if include_reversion:\n",
    "            is_reversion, reversion_error = check_predicted_output(predicted_output, other_output)\n",
    "        else:\n",
    "            is_reversion, reversion_error = None, None\n",
    "\n",
    "        execution_choice_counts['preference']['total'] += 1\n",
    "        if chosen_type == 'original':\n",
    "            execution_choice_counts['preference']['original'] += 1\n",
    "            bucket = execution_choice_counts['OC']\n",
    "        else:\n",
    "            execution_choice_counts['preference']['mutated'] += 1\n",
    "            bucket = execution_choice_counts['MC']\n",
    "\n",
    "        bucket['total'] += 1\n",
    "        if is_correct:\n",
    "            bucket['correct'] += 1\n",
    "        if include_reversion:\n",
    "            bucket['reversion_total'] += 1\n",
    "            if is_reversion:\n",
    "                bucket['reversion_correct'] += 1\n",
    "\n",
    "        run_record.update({\n",
    "            'chosen_program_letter': chosen_letter,\n",
    "            'chosen_program_type': chosen_type,\n",
    "            'assertion': assertion_text,\n",
    "            'prediction': predicted_output,\n",
    "            'correct_for_chosen_program': bool(is_correct),\n",
    "            'reversion_for_other_program': bool(is_reversion) if include_reversion else None,\n",
    "            'correctness_error': correctness_error,\n",
    "            'reversion_error': reversion_error,\n",
    "        })\n",
    "\n",
    "        execution_choice_results.append(run_record)\n",
    "\n",
    "\n",
    "def _safe_ratio(numerator: int, denominator: int) -> Optional[float]:\n",
    "    return None if denominator == 0 else numerator / denominator\n",
    "\n",
    "\n",
    "preference_total = execution_choice_counts['preference']['total']\n",
    "preference_original_rate = _safe_ratio(\n",
    "    execution_choice_counts['preference']['original'], preference_total\n",
    ")\n",
    "preference_mutated_rate = _safe_ratio(\n",
    "    execution_choice_counts['preference']['mutated'], preference_total\n",
    ")\n",
    "\n",
    "oc_correct_rate = _safe_ratio(\n",
    "    execution_choice_counts['OC']['correct'], execution_choice_counts['OC']['total']\n",
    ")\n",
    "or_reversion_rate = _safe_ratio(\n",
    "    execution_choice_counts['OC']['reversion_correct'], execution_choice_counts['OC']['reversion_total']\n",
    ")\n",
    "mc_correct_rate = _safe_ratio(\n",
    "    execution_choice_counts['MC']['correct'], execution_choice_counts['MC']['total']\n",
    ")\n",
    "mr_reversion_rate = _safe_ratio(\n",
    "    execution_choice_counts['MC']['reversion_correct'], execution_choice_counts['MC']['reversion_total']\n",
    ")\n",
    "\n",
    "execution_choice_summary = {\n",
    "    'dataset': 'LeetCode',\n",
    "    'problems_evaluated': len(problem_indices_choice),\n",
    "    'runs_per_problem': NUM_RUNS_PER_PROBLEM,\n",
    "    'preference_original': preference_original_rate,\n",
    "    'preference_mutated': preference_mutated_rate,\n",
    "    'oc_correct': oc_correct_rate,\n",
    "    'or_reversion': or_reversion_rate,\n",
    "    'mc_correct': mc_correct_rate,\n",
    "    'mr_reversion': mr_reversion_rate,\n",
    "    'avg_latency_s': (\n",
    "        sum(execution_choice_latencies) / len(execution_choice_latencies)\n",
    "        if execution_choice_latencies else None\n",
    "    ),\n",
    "    'invalid_runs': execution_choice_counts['invalid_runs'],\n",
    "    'reversion_skipped_problems': (\n",
    "        reversion_skip_count_choice if SKIP_BOOLEAN_FOR_REVERSION_CHOICE else 0\n",
    "    ),\n",
    "}\n",
    "\n",
    "execution_choice_table = pd.DataFrame([{\n",
    "    'Dataset': 'LeetCode',\n",
    "    'Problems Evaluated': execution_choice_summary['problems_evaluated'],\n",
    "    'Runs per Problem': execution_choice_summary['runs_per_problem'],\n",
    "    'Preference (Original)': execution_choice_summary['preference_original'],\n",
    "    'OC Correct': execution_choice_summary['oc_correct'],\n",
    "    'OR Reversion': execution_choice_summary['or_reversion'],\n",
    "    'MC Correct': execution_choice_summary['mc_correct'],\n",
    "    'MR Reversion': execution_choice_summary['mr_reversion'],\n",
    "}])\n",
    "\n",
    "choice_formatters = {\n",
    "    'Preference (Original)': lambda v, _pd=pd: 'N/A' if _pd.isna(v) else f\"{v:.2%}\",\n",
    "    'OC Correct': lambda v, _pd=pd: 'N/A' if _pd.isna(v) else f\"{v:.2%}\",\n",
    "    'OR Reversion': lambda v, _pd=pd: 'N/A' if _pd.isna(v) else f\"{v:.2%}\",\n",
    "    'MC Correct': lambda v, _pd=pd: 'N/A' if _pd.isna(v) else f\"{v:.2%}\",\n",
    "    'MR Reversion': lambda v, _pd=pd: 'N/A' if _pd.isna(v) else f\"{v:.2%}\",\n",
    "}\n",
    "\n",
    "display(execution_choice_table.style.format(choice_formatters))\n",
    "\n",
    "print('\n",
    "Preference counts:')\n",
    "print(\n",
    "    f\"  Original: {execution_choice_counts['preference']['original']} / {preference_total}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Mutated: {execution_choice_counts['preference']['mutated']} / {preference_total}\"\n",
    ")\n",
    "print(f\"Invalid runs (no usable JSON response): {execution_choice_counts['invalid_runs']}\")\n",
    "if execution_choice_summary['avg_latency_s'] is not None:\n",
    "    print(f\"Average latency per run: {execution_choice_summary['avg_latency_s']:.2f}s\")\n",
    "else:\n",
    "    print('Average latency per run: N/A')\n",
    "if SKIP_BOOLEAN_FOR_REVERSION_CHOICE:\n",
    "    print(\n",
    "        f\"Problems skipped for reversion metrics (boolean outputs): {reversion_skip_count_choice}\"\n",
    "    )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare Reasoning Efforts (Optional)\n",
    "\n",
    "Evaluate a small subset with different reasoning effort settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare low vs medium vs high reasoning\n",
    "reasoning_levels = [\"low\", \"medium\", \"high\"]\n",
    "comparison_results = {}\n",
    "\n",
    "NUM_COMPARISON_SAMPLES = 5  # Use small number for comparison\n",
    "MAX_NEW_TOKENS = 1000  # Allow enough tokens for reasoning + answer\n",
    "\n",
    "for reasoning_effort in reasoning_levels:\n",
    "    print(f\"\\nTesting reasoning_effort={reasoning_effort}...\")\n",
    "    correct = 0\n",
    "    total_latency = 0\n",
    "    \n",
    "    for idx in range(NUM_COMPARISON_SAMPLES):\n",
    "        sample = dataset['train'][idx]\n",
    "        \n",
    "        prompt = build_execution_prediction_prompt(sample)\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_dict=True,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "        ).to(model.device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        generated = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        latency = time.time() - start_time\n",
    "        total_latency += latency\n",
    "        \n",
    "        response = tokenizer.decode(\n",
    "            generated[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        predicted_output = extract_answer_from_response(response)\n",
    "        is_correct, _ = check_predicted_output(\n",
    "            predicted_output, sample['output']\n",
    "        )\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "    \n",
    "    comparison_results[reasoning_effort] = {\n",
    "        \"correct\": correct,\n",
    "        \"pass@1\": correct / NUM_COMPARISON_SAMPLES,\n",
    "        \"avg_latency\": total_latency / NUM_COMPARISON_SAMPLES\n",
    "    }\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REASONING EFFORT COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Reasoning':<12} {'pass@1':<10} {'Avg Latency':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for level, stats in comparison_results.items():\n",
    "    print(f\"{level:<12} {stats['pass@1']*100:>6.1f}%   {stats['avg_latency']:>10.2f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Increase NUM_SAMPLES** to evaluate on more problems\n",
    "2. **Try different reasoning_effort** levels\n",
    "3. **Compare with other models** (DeepSeek-R1, GPT-4o, etc.)\n",
    "4. **Analyze error patterns** to understand model weaknesses\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: To run on full dataset (347 samples), expect ~1-2 hours on free Colab."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}